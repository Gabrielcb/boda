CUCL_GLOBAL_KERNEL void %(rtc_func_name)( GASQ float const * const filts, // CUCL IN out_chan_blk:in_chan:y:x:out_chan_reg:out_chan_tile
					  GASQ float const * const biases, // CUCL IN out_chan
					  GASQ float const * const in, // CUCL IN blk_bline:blk_bx:blk_in_chan:blk_y:blk_x
					  GASQ float * const out, // CUCL OUT img:chan:y:x
					  uint32_t const flags ) // CUCL IN v
/* stride */  // CUCL REF y:x
/* in_pad */  // CUCL REF y:x 
/* work */  // CUCL REF blk_bline:blk_bx:out_chan_blk:blk_y:out_chan_tile:pels:out_chan
/* in_ref */  // CUCL REF img:chan:y:x
{
  // CUCL IX out_line out use_dims=img:y
  // CUCL IX filts_ix_out_chan_elem filts use_dims=in_chan:y:x
  // CUCL IX GRP_ID_1D work use_dims=blk_bline:blk_bx:out_chan_blk
  // CUCL IX LOC_ID_1D work use_dims=blk_y:out_chan_tile
  // note: <each thread handles> work use_dims=pels:out_chan; with pels_stride==out_chan_stride==t_tile_sz (currently); loops over in.chan==filts.in_chan

  LOCSHAR_MEM float all_smem[%(filts_smem_sz)+%(in_blk_in_chan_stride)]; // note: filts + in (or in detail: filts_smem_sz + in_blk_in_chan_stride)
  LSMASQ float * const filts_smem = all_smem;
  LSMASQ float * const in_smem = filts_smem + %(filts_smem_sz);
  float out_tile[%(work_pels_dim)*%(work_out_chan_dim)] = {0}; // tile of output for this thread to compute, stored in registers
  // reg. buffers for one strip each from in and filts, for the same filts_ix_out_chan_elem
  float filts_strip[%(work_out_chan_dim)]; // across output chans (stride is blk_filt_ix_sz )
  float in_strip[%(in_blk_x_dim)]; // segment of input line sufficient for one unrolling of inner loop

  int32_t blk_in_ix_base = %(GRP_ID_1D_blk_bx_nomod)*%(in_blk_bx_stride) + LOC_ID_1D;// index of first input pel to load for this thread
  int32_t filts_off = %(GRP_ID_1D_out_chan_blk)*%(filts_out_chan_blk_stride) + LOC_ID_1D; // index of first out chan (for this block) + LOC_ID_1D
  LSMASQ float * const filts_smem_off = filts_smem + %(LOC_ID_1D_out_chan_tile);

  int32_t out_line = %(GRP_ID_1D_blk_bline)*%(work_blk_y_dim); // first out_line of block
  int32_t const blk_fli = %(out_line_img); // image of first out_line of block
  out_line += %(LOC_ID_1D_blk_y); // adjust to out_line of this thread
  // offset in lines to deal with >1 img/block = (number of prior images (partial or full) in this block) * (adj to next img)
  int32_t const img_off_lines = (%(out_line_img) - blk_fli)*(%(filts_y_dim)-%(stride_y_dim)); 
  int32_t const in_y = %(out_line_y)*%(stride_y_dim) - %(in_pad_y_dim);
  if( flags == 3 ) { return; }

  for( int32_t in_chan = 0; in_chan != %(in_blk_in_chan_dim); ++in_chan ) {
    BARRIER_SYNC;
    %(in_smem_loads);
    for( int32_t ky = 0; ky != %(filts_y_dim); ++ky ) {
      if( ky != 0 ) { BARRIER_SYNC; }
      %(filts_smem_loads);
      BARRIER_SYNC;
      if( %(out_line_img) >= %(out_img_dim) ) { continue; } // required: skip lines from invalid images (read might be invalid)
      // FIXME: in_dim_1 --> in_ref_y_dim maybe? need to add in_ref REF dim?
      if( ((in_y+ky) < 0) || ((in_y+ky)>%(in_ref_y_dim)) ) { continue; } // optimization: skip known-to-be-padding input lines
      LSMASQ float * const in_smem_off = in_smem + (%(LOC_ID_1D_blk_y)*%(stride_y_dim)+ky+img_off_lines)*%(in_blk_y_stride);
      %(inner_loop_body);

    }
  }
  if( flags == 2 ) { return; }
  BARRIER_SYNC;
  %(biases_smem_loads);
  BARRIER_SYNC;
  %(bias_loads);
  if( flags == 1 ) { return; }
  %(stores);
}

